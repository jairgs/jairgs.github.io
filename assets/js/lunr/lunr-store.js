var store = [{
        "title": "Statistical Inference And Simulated Properties Of Some Statistics",
        "excerpt":" Abstract  In this document we present some aplications of statistical inference methods on real data. First we explore the most impotant theory on statistical inference, the Central Limit Theorem and show some difference between theoretical and sample statistics using an exponential distribution. Second, we explore the ToothGrowth database which features the results of an A/B test (a Randomized Experiment) on 60 guinea pigs to explore the effects of three different doses of vitamin C by one of two delivery methods on length of odontoblasts. Sample statistics properties through simulation and the Central Limit Theorem. At the very bottom of statistical inference, lies the definition of probability itself. Under the frequentist interpretation of probability, we assume the stochastic processes are repeatable and the probability of some outcome is the frecuency that outcome will appear in a large number of trials. The reality is, some processes are not repeatable and we are stuck with some limited realization of events. We rely on Statistical Inference in order to draw conclusions from sampled data making assumptions about the underlying distributions and derived properties of sample statistics. In this section we present simulated samples of size 40 from an exponential distribution fuction with \\(\\lambda\\) = 0.2 to study some properties of the sample mean and variance and how we can use inference to draw conclusions about the population. First we set the seed to make the simulations reproducible since we use pseudo random numbers. Then we plot 1000 random numbers from the underlying exponential function.  The previous plot simply plots the Exponential Density Function with \\(\\lambda\\) = 0.2 which has mean and standard deviation of 1/alpha. We see that the simulations are very close to the theoretical distribution as it was supposed to be. Next, we compute 1000 simulations of size 40 of the same underlying exponential distribution and compute the mean and variance of each sample. The Central Limit Theorem states that the mean of random variables has a limiting normal distribution with mean equal to the population mean and standard deviation equal to the population standard deviation divided by n squared, regardless of the underlying distribution. From our simulations we can plot the sample means and see how the distribution has changed.  We show with this histogram that the means follow closely a normal distribution function with mean 5 and standard deviation 5 (\\(1/\\lambda\\)), which is what the Central Limit Theroem formulates. The blue line represents the aforementioned normal distribution, the x axis plots the sample means for each simulation, while the red line shows that the histogram is centered at the population mean. We also plot the student’s distribution to show that, since we know the population variance is 1/lamda, the distribution of sample means is not as close as the normal distribution. Finnaly, the orange points show the theoretical exponential distribution with \\(\\lambda\\)=0.2 to show the sampled meand are not closely related to it, despite the fact that each sample come from it. Finally, we plot a histogram of the sample variances for each simulation to explore its distribution.  As we can see, the distribution of the sampled variances for each simulation is not related with the normal distribution function nor the underlying exponential distribution. The theoretical variance \\(1/\\lambda\\) is ploted as the red line and the sampled mean variance is plotted as the purple line. We can in general say that the sampled mean variance is a biased estimator of the variance, we can see this in the previous plot since the lines do not coincide. Statistical Inference Using the ThoothGrowth Data. This section develops some tests for the difference of means of groups to see the effects of different doses and administration methods of Vitamin C on tooth growth of guinea pigs. First we attach the data and call some summaries. ##       len        supp         dose      ##  Min.   : 4.20   OJ:30   Min.   :0.500  ##  1st Qu.:13.07   VC:30   1st Qu.:0.500  ##  Median :19.25           Median :1.000  ##  Mean   :18.81           Mean   :1.167  ##  3rd Qu.:25.27           3rd Qu.:2.000  ##  Max.   :33.90           Max.   :2.000This is a very simple data set and in order to make some exploratory analysis is sufficient to plot each pair of attributes in the data set.  The first inference from the data we are interested in doing is the estimation of the Average Treatment Effect of the two different delivery methods, orange juice and ascorbic acid. ## ##  Two Sample t-test## ## data:  ToothGrowth$len by ToothGrowth$supp## t = 1.9153, df = 58, p-value = 0.06039## alternative hypothesis: true difference in means is not equal to 0## 98.33333 percent confidence interval:##  -1.062765  8.462765## sample estimates:## mean in group OJ mean in group VC ##         20.66333         16.96333Since we are interested in difference of means, using the Central Limit Theorem and the fact that we are estimating the variance from the sampled variance, the difference of means between groups has a student’s t distribution with n-1 degrees of freedom, in this case 59. This result, does not make any assumption about the underlying distribution from which the data was drawn. In our test, we are defining a two sided test because we are not making assumptions about which group should have greater mean. Our mu argument is equal to zero because the difference of means is zero under the null hipothesis. The data is unpaired because each group is a randomized sample of different pigs. We are assuming equal variance between the groups since the subjects are randomized and there is no reason to believe otherwise. The confidence level is selected with a corrected Type I error rate according with the Bonferroni Correction. This correction is needed since we are making three tests from the same data realization and the probablity of drawing a wrong conclusion aggregates with the ammount of tests; there is only so much inference we can draw from one set of data. The Bonferroni correction is nothing more than the significance level divided by the number of tests you are doing. From the previous test, we cannot reject the null hipothesis of equal means on the effect of orange juice and ascorbic acid on the length of odontoblasts, even when the sample means are very different. We can almost see it from the initial plot since the variance of ascorbic acid is very large with respect to the variance of orange juice. Next we make two test comparing the first dosage level with the second and the second with the third using the same parameters as we used in the first test. ## ##  Two Sample t-test## ## data:  ToothGrowth$len[ToothGrowth$dose == 0.5] and ToothGrowth$len[ToothGrowth$dose == 1]## t = -6.4766, df = 38, p-value = 1.266e-07## alternative hypothesis: true difference in means is not equal to 0## 98.33333 percent confidence interval:##  -12.660704  -5.599296## sample estimates:## mean of x mean of y ##    10.605    19.735## ##  Two Sample t-test## ## data:  ToothGrowth$len[ToothGrowth$dose == 1] and ToothGrowth$len[ToothGrowth$dose == 2]## t = -4.9005, df = 38, p-value = 1.811e-05## alternative hypothesis: true difference in means is not equal to 0## 98.33333 percent confidence interval:##  -9.618121 -3.111879## sample estimates:## mean of x mean of y ##    19.735    26.100We can infer from the data that the second group has a statistically significantly bigger mean than group one and the third group has a mean bigger than the group two. Conclusion In conclusion, from this randomized experiments on Guinea Pigs we observed that while the administration method has no impact on the length of odontoblasts, the dose levels has a statistically significant impact on the same outcome with the expected direction. ","categories": [],
        "tags": ["Data Science","Statistics","Inference","Simulation"],
        "url": "https://jairgs.github.io/Statistical-Inference-and-Simulated-Properties-of-some-statistics/",
        "teaser":null},{
        "title": "Principal Meteorological Events Responsible Of Fatalities, Injuries And Property Damage",
        "excerpt":"         Abstract   Some recent interest in meteorological events and their economic and health consequences motivate this quantitative analysis to identify such events in order of importance and quantify its economic and health effects on the general population. We find that Hurricanes are the most expensive events with respect to property damage costing $733 Million US Dollars each, Typhoons are associated with the biggest number of injuries with 13.38 on average and Tsunamis take the most lives of all meteorological events with 1.65 persons in average. Introduction In order to alleviate or even prevent future economic damages and health problems in the general population caused by catastrophic events (e.g. Hurricanes and Typhoons), it is of the most importance to quantify the effects of each and every type of event in order to not only asses the absolute impact but also its impact in relation to each other. Are Hurricanes more fatal than Tsunamis? Are Typhoons more expensive than Floods? These types of questions motivate our present analysis. In concrete, we are interested in ranking and quantifying the effects of every type of catastrophic event in the NATIONAL WEATHER SERVICE data base with respect to property damage in US dollars, fatalities and injuries, both in units. The documents is dived in an introduction motivating the questions, a data processing section describing the data set and the appropriate code to make it usable for our purposes and also discuses some of the limitations of the data set, an analysis section describing the methodology applied and a results section which presents the conclusions of the analysis and mentions possible limitations and extensions of the analysis. Data Processing In this section we present the code to make the data analytical for our purposes and also present some descriptives to spot some main problems and limitations of the data. The processing code bellow shows the necessary instruction to load and summarize the code. ##     STATE__                  BGN_DATE             BGN_TIME##  Min.   : 1.0   5/25/2011 0:00:00:  1202   12:00:00 AM: 10163##  1st Qu.:19.0   4/27/2011 0:00:00:  1193   06:00:00 PM:  7350##  Median :30.0   6/9/2011 0:00:00 :  1030   04:00:00 PM:  7261##  Mean   :31.2   5/30/2004 0:00:00:  1016   05:00:00 PM:  6891##  3rd Qu.:45.0   4/4/2011 0:00:00 :  1009   12:00:00 PM:  6703##  Max.   :95.0   4/2/2006 0:00:00 :   981   03:00:00 PM:  6700##                 (Other)          :895866   (Other)    :857229##    TIME_ZONE          COUNTY           COUNTYNAME         STATE##  CST    :547493   Min.   :  0.0   JEFFERSON :  7840   TX     : 83728##  EST    :245558   1st Qu.: 31.0   WASHINGTON:  7603   KS     : 53440##  MST    : 68390   Median : 75.0   JACKSON   :  6660   OK     : 46802##  PST    : 28302   Mean   :100.6   FRANKLIN  :  6256   MO     : 35648##  AST    :  6360   3rd Qu.:131.0   LINCOLN   :  5937   IA     : 31069##  HST    :  2563   Max.   :873.0   MADISON   :  5632   NE     : 30271##  (Other):  3631                   (Other)   :862369   (Other):621339##                EVTYPE         BGN_RANGE           BGN_AZI##  HAIL             :288661   Min.   :   0.000          :547332##  TSTM WIND        :219940   1st Qu.:   0.000   N      : 86752##  THUNDERSTORM WIND: 82563   Median :   0.000   W      : 38446##  TORNADO          : 60652   Mean   :   1.484   S      : 37558##  FLASH FLOOD      : 54277   3rd Qu.:   1.000   E      : 33178##  FLOOD            : 25326   Max.   :3749.000   NW     : 24041##  (Other)          :170878                      (Other):134990##          BGN_LOCATI                  END_DATE             END_TIME##               :287743                    :243411              :238978##  COUNTYWIDE   : 19680   4/27/2011 0:00:00:  1214   06:00:00 PM:  9802##  Countywide   :   993   5/25/2011 0:00:00:  1196   05:00:00 PM:  8314##  SPRINGFIELD  :   843   6/9/2011 0:00:00 :  1021   04:00:00 PM:  8104##  SOUTH PORTION:   810   4/4/2011 0:00:00 :  1007   12:00:00 PM:  7483##  NORTH PORTION:   784   5/30/2004 0:00:00:   998   11:59:00 PM:  7184##  (Other)      :591444   (Other)          :653450   (Other)    :622432##    COUNTY_END COUNTYENDN       END_RANGE           END_AZI##  Min.   :0    Mode:logical   Min.   :  0.0000          :724837##  1st Qu.:0    NA's:902297    1st Qu.:  0.0000   N      : 28082##  Median :0                   Median :  0.0000   S      : 22510##  Mean   :0                   Mean   :  0.9862   W      : 20119##  3rd Qu.:0                   3rd Qu.:  0.0000   E      : 20047##  Max.   :0                   Max.   :925.0000   NE     : 14606##                                                 (Other): 72096##            END_LOCATI         LENGTH              WIDTH##                 :499225   Min.   :   0.0000   Min.   :   0.000##  COUNTYWIDE     : 19731   1st Qu.:   0.0000   1st Qu.:   0.000##  SOUTH PORTION  :   833   Median :   0.0000   Median :   0.000##  NORTH PORTION  :   780   Mean   :   0.2301   Mean   :   7.503##  CENTRAL PORTION:   617   3rd Qu.:   0.0000   3rd Qu.:   0.000##  SPRINGFIELD    :   575   Max.   :2315.0000   Max.   :4400.000##  (Other)        :380536##        F               MAG            FATALITIES          INJURIES##  Min.   :0.0      Min.   :    0.0   Min.   :  0.0000   Min.   :   0.0000##  1st Qu.:0.0      1st Qu.:    0.0   1st Qu.:  0.0000   1st Qu.:   0.0000##  Median :1.0      Median :   50.0   Median :  0.0000   Median :   0.0000##  Mean   :0.9      Mean   :   46.9   Mean   :  0.0168   Mean   :   0.1557##  3rd Qu.:1.0      3rd Qu.:   75.0   3rd Qu.:  0.0000   3rd Qu.:   0.0000##  Max.   :5.0      Max.   :22000.0   Max.   :583.0000   Max.   :1700.0000##  NA's   :843563We are intersted in the EVTYPE, INJURIES, FATALITIES and PROPDMG, the last one needing some processing to convert in US dollars according to the variable PROPDMGEXP which is supposed to have three levels of magnitude: K for thusands, M for millions and B for billions. We observe a samll number of obervations in other levels which we are going to assume are US dollars for simplicity. The next code describes the transformation of PROPDMG to a new variable in US Dollars. Lets zoom to the EVTYPE variable. ####             HIGH SURF ADVISORY                  COASTAL FLOOD##                              1                              1##                    FLASH FLOOD                      LIGHTNING##                              1                              1##                      TSTM WIND                TSTM WIND (G45)##                              4                              1##                     WATERSPOUT                           WIND##                              1                              1##                              ?                ABNORMAL WARMTH##                              1                              4##                 ABNORMALLY DRY                 ABNORMALLY WET##                              2                              1##           ACCUMULATED SNOWFALL            AGRICULTURAL FREEZE##                              4                              6##                  APACHE COUNTY         ASTRONOMICAL HIGH TIDE##                              1                            103##          ASTRONOMICAL LOW TIDE                       AVALANCE##                            174                              1##                      AVALANCHE                   BEACH EROSIN##                            386                              1##                  Beach Erosion                  BEACH EROSION##                              1                              3##    BEACH EROSION/COASTAL FLOOD                    BEACH FLOOD##                              1                              2##     BELOW NORMAL PRECIPITATION              BITTER WIND CHILL##                              2                              1## BITTER WIND CHILL TEMPERATURES                      Black Ice##                              3                              3##                      BLACK ICE                       BLIZZARD##                             14                           2719## BLIZZARD AND EXTREME WIND CHIL        BLIZZARD AND HEAVY SNOW##                              2                              1##               Blizzard Summary               BLIZZARD WEATHER##                              1                              1##         BLIZZARD/FREEZING RAIN            BLIZZARD/HEAVY SNOW##                              1                              2##             BLIZZARD/HIGH WIND          BLIZZARD/WINTER STORM##                              1                              1##                  BLOW-OUT TIDE                 BLOW-OUT TIDES##                              1                              1##                   BLOWING DUST                   blowing snow##                              4                              2##                   Blowing Snow                   BLOWING SNOW##                              3                             12## BLOWING SNOW- EXTREME WIND CHI BLOWING SNOW &amp; EXTREME WIND CH##                              1                              2## BLOWING SNOW/EXTREME WIND CHIL               BREAKUP FLOODING##                              1                              1##                     BRUSH FIRE                    BRUSH FIRES##                              3                              1As we can see from the first rows of the Event Type column, there is no homogeneity in the way each event is described (possibly because it was typed by each weather observatory without following strict definition of events or human errors). This is one of the most important descriptor of the database and crucial for our intended analysis. The ideal way in which we need the Event Type column to appear is in the form of a factor variable with each event type as a distinct level. To accompish this task we first import the description key for the events which was constructed from the table 2.1.1 “Storm Data Event Table” from page 6 of the Storm Data Preparation document available at https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf Our strategy for building our desired vector of events with standarized description of events is gonna be the following: Split dual events in the key table, we are gonna merge them together in the text matching algotighm (see below). Note that some events are gonna be true in more than one event variable; this is because sometimes the event type matches more than one event description from the table.Import the key as a character vector.construct a loop that matches every element of the key vector to the EVTYPE column and construct a new dummy variable based on the matching result.Lets import the key and take a look at the first elements. ##       Event.Description## 1 Astronomical Low Tide## 2             Avalanche## 3              Blizzard## 4         Coastal Flood## 5                  Cold## 6           Debris FlowLet’s create the events dataframe with logical elements. Now let’s create the composed variables of the original key table (i.e. Cold/Wind Chill, Extreme Cold/Wind Chill, Frost/Freeze , Hurricane (Typhoon) and Storm Surge/Tide). Note that the only event that intersect with others is “Wind Chill”, therefore is the only one we need to delete. Finally, we converted the logical data frame to binary in order to treat each column as a dummy variable. (In this case, turns out this transformation is necessary in order to run the dummy regression with no intersect.) This last data frame is what we need in order to run our methodology. Analysis Our strategy to quantify the effects of each event in mortality, injuries and property damage is very straightforward. We are going to fit a Classic Linear Model to each of our three outcomes: injuries, fatalities and property damage with all our dummy variables as covariates. We assume the classic assumptions of normality and finite variance hold for the error term. Each regression line is gonna be fitted without intercept in order to have each coefficient represent the estimated population mean and also to directly interpret each p value as the probability of observing the coefficients magnitude or larger under the null hypothesis of mean equal to 0 (\\(H_0:\\beta i=0|H_1:\\beta i\\neq0\\) where i is each dummy variable we created representing the events.) Once we train the linear model, we are gonna grab the coefficients and plot them in descending order to see which ones are the biggest in each regression and also make some comments about its statistical significance and the magnitud of the damage. Results In this section we present the main results and plots generated. The first regression corresponing to the fatalities outcome and the main plot is presented in the following figure.  The variable Debris.Flow presents singularity because it doesnt have any variability. It is important to note the some variables are not statistically significantly different from zero under a two sided t test for every standard significance level. We are, however, most interested in the biggest one in order of magnitud. The main three regressors in order of magnitud to explain the number of fatalities are Tsunami, Heat and Rip. Current all three strongly statistically different from zero. Tsunamis for example, have a 1.65 point estimate meaning each event causes approximately 1.65 deaths. Next we review the injuries estiamtion.  The results show that Typhoons, Hurricanes and Tsunamis in that order are the main events related to injuries. All of them show statistically different from zero estimates and for example, each Typhoon event corresponds to 13.38 injuries in average. Finnaly, the results from the estimation of the property damage outcome is shown in the following lines.  The estimation show that the top three most devastating meteorological events for property damages are Hurricanes, Typhoons and Storm Surge Tides in that order with its estimators strongly different from zero after accounting for sources of variation. Hurricanes cost $733 Million US Dollars each on average. Final Remarks To end the analysis and open the discussion we think more detail in the analysis for example separating estimations by state can lead to useful insights since the marginal effects of each type of catastrophic event may differ between states due to the intrinsic climatological conditions of said places. Another interesting aspect of the data to further investigate is the count nature of injuries and fatalities which may be far from the assumption of normality because they are bounded at zero and also not continuous, maybe not even asyptotically. A poisson distribution of \\(Y-E(Y|X)\\) might be more appropriate.                        ","categories": [],
        "tags": ["Meteorology","Regression","Data Science","Plots"],
        "url": "https://jairgs.github.io/Principal-meteorological-events-responsible-of-fatalities,-injuries-and-property-damage/",
        "teaser":null},{
        "title": "Regression Analysis: a study of the effect of transmission type to gasoline consumption in cars.",
        "excerpt":" Executive Summary  Regression analysis is one of the most important tools for a data scientist and statisticians in general since it provides a simple yet insightful way to model the underlying dynamics of the data. In this analysis, we show some strategies to model the Miles/gallon as a function of transmission type using regression analysis in order to see wheter there is a significant relationship between the two. We also explore some tools of regression to make inferential conclusions about the population. The analysis concludes that while there is a positive relationship between manual transmission and gas consumption, this effect is not statistically different from zero. More analysis needs to be done perhaps with a bigger data set to more precisely estimate this effect. Introduction Regression analysis is one of the most important tools for a data scientist and statisticians in general since it provides a simple yet insightful way to model the underlying dynamics of the data. Regression analysis provides an intuitive and easy to understand framework to make population inference, meassure uncertainty, develop predictions and meassure partialized effects of variables. If we are prepared to mathematically complicate the computations at the cost of interpretability, generalized regression methods can provide a wide veriety of families of distributions that can be used to model the data. In this brief analysis of the data from the 1974 Motor Trend US magazine, which comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973–74 models), we show some of the advantages of using regression models. The attributes in the data have the following descriptions: 1. mpg Miles/(US) gallon 2. cyl Number of cylinders 3. disp Displacement (cu.in.) 4. hp Gross horsepower 5. drat Rear axle ratio 6. wt Weight (1000 lbs) 7. qsec 1/4 mile time 8. vs V/S 9. am Transmission (0 = automatic, 1 = manual) 10. gear Number of forward gears 11. carb Number of carburetors Regression Analysis First we are going to load the data set and make some summaries. ##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1There are only 32 observations in the data, this is gonna be important since the variance of our estimators depend on the number of observations, ergo we expect our estimated variances to be inflated. We observe there are some categorical variables in the data coded as numeric, for instance the cylinder number and the carburetors number. Normally we would recode them as factors since we are interested in the patial effect of each level in the mpg; however each level minus the reference level enters as one variable in the regression model and we don’t want to reduce our degrees of freedom even more. Recall that the degrees of freedom of the t statistics is equal to \\(n-k\\) where \\(n\\) is the number of observations and \\(k\\) is the number of estimators includind the intercept. There is no assuption about the distribution of our covariates that limits us from running these variables as numeric. We are interested in wether the gasoline consumpion in mpg has a statistically significant relationship with the transmission type. To answer this queston we first need to inform ourselves about the sample distribution of mpg, our outcome variable.  We can see that the sample distribution is not skewed and it is reasonable to assume it comes from a normal distribution. In fact, in order to fit a Classic Linear Model, we need the error term \\(mpg_i - E(mpg|X_i)\\)~\\(N(0, \\sigma ^2)\\) and that seems to be a reasonable assumption for this sample distribution, the reality is, we can’t test it, that is an assumption we have to make. Let’s place some labels to identify the levels of the transmission type and plot our variables of interest, mpg and am.  From the previos plot, it seems there exists some linear relationship between gas consumption and transmission type. Let’s run our linear model. ## ## Call:## lm(formula = mpg ~ am, data = mtcars)## ## Coefficients:## (Intercept)     ammanual  ##      17.147        7.245There is a statistically significant effect of the transmission type on the gas consumption. In fact, manual transmission seems to increase the gas consumption by 7.245 Miles/gallon on average with a probability of Type I error significantly low at acceptable levels. In order to make a unbiased conclusion about the effect of transmission type on gas consumption, the Linear Regression Model assumes that \\(Cov(error, transmission type) = 0\\) which means that there are not other relevant variables determining mpg that also are related to the transmission type. This is a very strong assumption and it is not realistic in our framework. Some manufacturers may opt to construct the most powerful cars with a specific type of transmission or automatic transmission may cause the car to be heavier, both of which variables are related to gas consumption. Our next strategy is to use every variable in the data set to construct our model. ## ## Call:## lm(formula = mpg ~ ., data = mtcars)## ## Coefficients:## (Intercept)          cyl         disp           hp         drat  ##    12.30337     -0.11144      0.01334     -0.02148      0.78711  ##          wt         qsec           vs     ammanual         gear  ##    -3.71530      0.82104      0.31776      2.52023      0.65541  ##        carb  ##    -0.19942In this estimation none of the estimators is significant at 5% significance level. With 21 degrees of freedom left, it is very difficult to precisely estimate the variance estimators of our variables, let alone the estimators of the effects themselves. We decided to compute the variance inflation factors of each variable to see if there are some variables that are being explained by a linear combination of the others. Normally we wouldn’t need to do this unless there is some variables highly correlated with the transmission type, but we are aiming for frugality in our strategy. ##      drat        am        vs      gear      qsec      carb        hp ##  3.374620  4.648487  4.965873  5.357452  7.527958  7.908747  9.832037 ##        wt       cyl      disp ## 15.164887 15.373833 21.620241The displacement and number of cylinders have high vif and we can hipothesize that the displacement is already explained by the number of cylinders, the horsepower and the weight, the same as the other variables with high vif are explained by others. We decided to drop these three variables and fit our linear model once again. ## ## Call:## lm(formula = mpg ~ . - disp - cyl, data = mtcars)## ## Coefficients:## (Intercept)           hp         drat           wt         qsec  ##    13.80810     -0.01225      0.88894     -2.60968      0.63983  ##          vs     ammanual         gear         carb  ##     0.08786      2.42418      0.69390     -0.61286We end up with 23 degrees of freedom and now one of our variables is statistically significant at the 5% significance level in a two sided t test. The high number of insignificant variables is still a concern with this model since the degrees of freedom is still very low. We decided to follow a different approach into modeling which variables may be relevant for the model. We already know by general knowledge that mpg is caused by most if not all of our variables in our data set but in order to not violate our assuption we need to identify which ones are likely related with the transmission type in our population. To give some light on this, we plotted a scatter plot of transmission type vs all the variables in our data set excluding the ones we already discarded and, of course, mpg.  It seems from the plots that, at least in the sample, the transmission type is related with weight, seconds to querter mile, number of carburetors and the number of gears. We then fit our model with these variables. ## ## Call:## lm(formula = mpg ~ wt + qsec + carb + gear + am, data = mtcars)## ## Residuals:##     Min      1Q  Median      3Q     Max ## -4.1008 -1.4091 -0.1297  1.2894  4.3129 ## ## Coefficients:##             Estimate Std. Error t value Pr(&gt;|t|)   ## (Intercept)  10.9012     8.0844   1.348  0.18915   ## wt           -3.1456     0.9283  -3.389  0.00225 **## qsec          0.9507     0.3553   2.676  0.01274 * ## carb         -0.7094     0.5328  -1.332  0.19457   ## gear          0.8588     1.2477   0.688  0.49735   ## ammanual      2.8799     1.7602   1.636  0.11387   ## ---## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## ## Residual standard error: 2.468 on 26 degrees of freedom## Multiple R-squared:  0.8594, Adjusted R-squared:  0.8323 ## F-statistic: 31.77 on 5 and 26 DF,  p-value: 2.761e-10Weight again is significantly different from zero now at 99% Type I error and qsec is now relevant at 98% significance level. Our variable of interest transmission type is still not statistically significant. This might be our final specification. Let’s quickly plot some residual plots.  From the Residuals vs Fitted plot it seems there might be some heteroskedasticity in the data but not very strong and combined with the small number of observations, we think there is not enough evidence of this. The normal Q-Q seems to strenghten our assumption of normality in the residuals, making this Linear Model ideal. In the residuals vs leverage plot, we can see that there are two cars, Merc 230 and Ford Pantera L with high leverage and we can say they are using their leverage because the standarized residuals are between -1 and -2. We took out those two points to see what happens with the estimators and their variances. ## ## Call:## lm(formula = mpg ~ wt + qsec + carb + gear + am, data = mtcars[!row.names(mtcars) == ##     &quot;Merc 230&quot; &amp; !row.names(mtcars) == &quot;Ford Pantera L&quot;, ])## ## Coefficients:## (Intercept)           wt         qsec         carb         gear  ##      7.0587      -2.6054       0.8817      -1.0048       2.1076  ##    ammanual  ##      2.3644While the estimators are changing, our transmission type variables continues to be statistically not signifficant which means its 95% and even 90% confidence intervals touch the zero. We then decided to keep the last model without droping the two outliers as our final model.  Conclusion Let’s finally make a confidence interval for our variable of interest.##                  2.5 %     97.5 %## (Intercept) -5.7165450 27.5188704## wt          -5.0537402 -1.2375448## qsec         0.2203138  1.6810164## carb        -1.8044745  0.3857182## gear        -1.7058344  3.4234115## ammanual    -0.7383426  6.4980468Our transmission type point estimation is 2.8799 which means the average manual car has a gas consumption 2.88 Miles/gallon higher than the average automatic car, controling for weight, seconds to querter mile, number of carburetors and the number of gears. The confidence interval at 95% level is [-0.738, 6.498] for the change from automatic to manual which means we cannot reject the null hipothesis that this change is zero. ","categories": [],
        "tags": ["Inference","Regression","Weights"],
        "url": "https://jairgs.github.io/Regression-Analysis/",
        "teaser":null},{
        "title": "Machine Lerning Methods: an application building a correct dumbell curls classifier",
        "excerpt":"Introduction With the advent of devices such as Jawbone Up, Nike FuelBand and Fitbit, it is increasibly easy and cheap for people to monitor and report body movements in order to improve their health, find patterns in their habits and just because is fun. With this devices comes an increasingly large and interesting bank of data on body movements which we can use in order to classify a variety of daily tasks performed by humans. This classifiers can be built in order to predict what the person is doing in real time and detonate a wide number of useful events, for example, play some music according to the kind of task, start caloric estimation counts, show specific ads, etc. One interesting event would be for the device to understand you are working out and predict what kind of workout you are doing in order to monitor whether you are doing it wrong and give an alert in order to prevent lesions. In this document, we analyise the Weight Lifting Exercise Dataset1 in order to build a classifier to predict if the excersise is being done corrrectly or not. Each subject was asked to perform a Dumbbell Biceps Curl excersise in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). We built 5 classifiers using different Machine Learning Techniques with cross validation of order 5, each one using the same kind of classifier either by bagging or boosting techinques and blended them together with a simple mayority blender. Data set processing We read the RAW dataset and show some summary statistics of the first 20 columns. training &lt;- read.csv(&quot;train.csv&quot;)summary(training[,1:20])##        X            user_name    raw_timestamp_part_1 raw_timestamp_part_2##  Min.   :    1   adelmo  :3892   Min.   :1.322e+09    Min.   :   294      ##  1st Qu.: 4906   carlitos:3112   1st Qu.:1.323e+09    1st Qu.:252912      ##  Median : 9812   charles :3536   Median :1.323e+09    Median :496380      ##  Mean   : 9812   eurico  :3070   Mean   :1.323e+09    Mean   :500656      ##  3rd Qu.:14717   jeremy  :3402   3rd Qu.:1.323e+09    3rd Qu.:751891      ##  Max.   :19622   pedro   :2610   Max.   :1.323e+09    Max.   :998801      ##                                                                           ##           cvtd_timestamp  new_window    num_window      roll_belt     ##  28/11/2011 14:14: 1498   no :19216   Min.   :  1.0   Min.   :-28.90  ##  05/12/2011 11:24: 1497   yes:  406   1st Qu.:222.0   1st Qu.:  1.10  ##  30/11/2011 17:11: 1440               Median :424.0   Median :113.00  ##  05/12/2011 11:25: 1425               Mean   :430.6   Mean   : 64.41  ##  02/12/2011 14:57: 1380               3rd Qu.:644.0   3rd Qu.:123.00  ##  02/12/2011 13:34: 1375               Max.   :864.0   Max.   :162.00  ##  (Other)         :11007                                               ##    pitch_belt          yaw_belt       total_accel_belt kurtosis_roll_belt##  Min.   :-55.8000   Min.   :-180.00   Min.   : 0.00             :19216   ##  1st Qu.:  1.7600   1st Qu.: -88.30   1st Qu.: 3.00    #DIV/0!  :   10   ##  Median :  5.2800   Median : -13.00   Median :17.00    -1.908453:    2   ##  Mean   :  0.3053   Mean   : -11.21   Mean   :11.31    -0.016850:    1   ##  3rd Qu.: 14.9000   3rd Qu.:  12.90   3rd Qu.:18.00    -0.021024:    1   ##  Max.   : 60.3000   Max.   : 179.00   Max.   :29.00    -0.025513:    1   ##                                                        (Other)  :  391   ##  kurtosis_picth_belt kurtosis_yaw_belt skewness_roll_belt##           :19216            :19216              :19216   ##  #DIV/0!  :   32     #DIV/0!:  406     #DIV/0!  :    9   ##  47.000000:    4                       0.000000 :    4   ##  -0.150950:    3                       0.422463 :    2   ##  -0.684748:    3                       -0.003095:    1   ##  -1.750749:    3                       -0.010002:    1   ##  (Other)  :  361                       (Other)  :  389   ##  skewness_roll_belt.1 skewness_yaw_belt max_roll_belt     max_picth_belt ##           :19216             :19216     Min.   :-94.300   Min.   : 3.00  ##  #DIV/0!  :   32      #DIV/0!:  406     1st Qu.:-88.000   1st Qu.: 5.00  ##  0.000000 :    4                        Median : -5.100   Median :18.00  ##  -2.156553:    3                        Mean   : -6.667   Mean   :12.92  ##  -3.072669:    3                        3rd Qu.: 18.500   3rd Qu.:19.00  ##  -6.324555:    3                        Max.   :180.000   Max.   :30.00  ##  (Other)  :  361                        NA's   :19216     NA's   :19216  ##   max_yaw_belt  ##         :19216  ##  -1.1   :   30  ##  -1.4   :   29  ##  -1.2   :   26  ##  -0.9   :   24  ##  -1.3   :   22  ##  (Other):  275There are several reading intervals from 0.5 to 2.5 seconds with 0.5 seconds overlaping and for each of these intervals, some transformations were developed and reported such as the kourtosis and Euler angles. Several of these indicators unfortunately, had either no variation or most of the samples were NA’s. We performed some basic data cleaning in order to keep only statistically useful attributes to build our classifier. training &lt;- training[,-grep(&quot;skewness|kurtosis|yaw|pitch|roll|picth|var&quot;,names(training))]Also we got rid of variables such as the timestamps which were not particulary useful to build the prediction algorithm but kept important attributes such as the window number of the reading and the user name performing the task since each person may perform the dumbell curl slightly differently and we want to account for that variation. training &lt;- training[,c(2,7:48)]This is the final training data set in which we built our classifier. Training Phase To build this classifier, we didn’t performed a data split because: We avoided overfitting, performed model selection and tuned each classifier using 5-fold cross validation.The out of sample accuracy measurement is not of the outmost importance to us since it will be provided by a web algorithm using the provided test set (which has no outcome variable class).We trained 6 Machine Learning algorithms: Linear discriminant analysisPenalized Multinomial RegressionModel Averaged Neural NetworkNaive BayesRandom ForestsBayesian Generalized Linear ModelUsing the cross validated accuracy parameters, we decided that we were not using the Model Averaged Neural Network because it did not fit the data very well. The other 5 classifiers were used in the majority vote function. set.seed(574)z &lt;- trainControl(method = &quot;cv&quot;, number = 5)fit1 &lt;- train(classe~., data=training, method=&quot;lda&quot;, trControl=z)fit2 &lt;- train(classe~., data=training, method=&quot;multinom&quot;, trControl=z)fit3 &lt;- train(classe~., data=training, method=&quot;avNNet&quot;, trControl=z)fit6 &lt;- train(classe~., data=training, method=&quot;nb&quot;, trControl=z)fit9 &lt;- train(classe~., data=training, method=&quot;rf&quot;, trControl=z)fit10 &lt;- train(classe~., data=training, method=&quot;bayesglm&quot;, trControl=z)Other models were discarded beacause of the computational time contraint. Results In this section we show the cross validated accuracy estimators and build the blending algorithm to predict on the testing set and feedt the predictions to the web based app. fit1$results##   parameter  Accuracy     Kappa  AccuracySD     KappaSD## 1      none 0.6920287 0.6089305 0.003304142 0.004077175fit2$results##   decay  Accuracy     Kappa  AccuracySD    KappaSD## 1 0e+00 0.6317905 0.5345320 0.008711726 0.01027731## 2 1e-04 0.6317905 0.5345320 0.008711726 0.01027731## 3 1e-01 0.6318414 0.5345976 0.008754426 0.01033499fit6$results##   usekernel fL adjust  Accuracy     Kappa  AccuracySD    KappaSD## 1     FALSE  0      1 0.4267021 0.2945092 0.043800363 0.04386220## 2      TRUE  0      1 0.7211282 0.6422316 0.007378415 0.01144043fit9$results##   mtry  Accuracy     Kappa   AccuracySD      KappaSD## 1    2 0.9923557 0.9903296 0.0016599164 0.0020999124## 2   24 0.9967895 0.9959389 0.0007760752 0.0009816961## 3   46 0.9946492 0.9932312 0.0013817935 0.0017480006fit10$results##   parameter  Accuracy     Kappa  AccuracySD    KappaSD## 1      none 0.3902767 0.2203644 0.003932735 0.00461844We can see that the best accuracy comes from the random forest algorithm with a final mtry of 24 to provide a final cross validated accuracy of 99.64% and \\(\\kappa\\) = 99.59%. The rest of the classifiers are not very good by their own but we can use them to build an out of sample superior classification algorithm than any of the calssifiers alone. predictions &lt;- data.frame(lda=predict(fit1, testing), multinom=predict(fit2, testing), nb=predict(fit6, testing), rf=predict(fit9, testing), bayesglm=predict(fit10, testing))counts &lt;- matrix(nrow = 20,ncol = 5)for (i in 1:5){    counts[,i] &lt;- apply(predictions, 1, function(x) sum(x==LETTERS[i]))}counts &lt;- data.frame(counts)names(counts)&lt;-LETTERS[1:5]finalPrediction &lt;- names(counts)[max.col(counts)]This blended final prediction is gonna be fed to the web-based code to build the out of sample accuracy rate but that is out of the scope of the present analysis. References 1: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13). Stuttgart, Germany: ACM SIGCHI, 2013. Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4WCwxc9hG 2: Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, Brenton Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau, Andrew Ziem, Luca Scrucca, Yuan Tang, Can Candan and Tyler Hunt. (2016). caret: Classification and Regression Training. R package version 6.0-73. https://CRAN.R-project.org/package=caret Annex Overview of the tuning algorithms and the final optimal parameters fit1## Linear Discriminant Analysis ## ## 19622 samples##    42 predictor##     5 classes: 'A', 'B', 'C', 'D', 'E' ## ## No pre-processing## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 15695, 15698, 15699, 15699, 15697 ## Resampling results:## ##   Accuracy   Kappa    ##   0.6920287  0.6089305## ## fit2## Penalized Multinomial Regression ## ## 19622 samples##    42 predictor##     5 classes: 'A', 'B', 'C', 'D', 'E' ## ## No pre-processing## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 15698, 15699, 15696, 15697, 15698 ## Resampling results across tuning parameters:## ##   decay  Accuracy   Kappa    ##   0e+00  0.6317905  0.5345320##   1e-04  0.6317905  0.5345320##   1e-01  0.6318414  0.5345976## ## Accuracy was used to select the optimal model using  the largest value.## The final value used for the model was decay = 0.1.fit6## Naive Bayes ## ## 19622 samples##    42 predictor##     5 classes: 'A', 'B', 'C', 'D', 'E' ## ## No pre-processing## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 15699, 15698, 15698, 15698, 15695 ## Resampling results across tuning parameters:## ##   usekernel  Accuracy   Kappa    ##   FALSE      0.4267021  0.2945092##    TRUE      0.7211282  0.6422316## ## Tuning parameter 'fL' was held constant at a value of 0## Tuning##  parameter 'adjust' was held constant at a value of 1## Accuracy was used to select the optimal model using  the largest value.## The final values used for the model were fL = 0, usekernel = TRUE##  and adjust = 1.fit9## Random Forest ## ## 19622 samples##    42 predictor##     5 classes: 'A', 'B', 'C', 'D', 'E' ## ## No pre-processing## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 15698, 15698, 15699, 15695, 15698 ## Resampling results across tuning parameters:## ##   mtry  Accuracy   Kappa    ##    2    0.9923557  0.9903296##   24    0.9967895  0.9959389##   46    0.9946492  0.9932312## ## Accuracy was used to select the optimal model using  the largest value.## The final value used for the model was mtry = 24.fit10## Bayesian Generalized Linear Model ## ## 19622 samples##    42 predictor##     5 classes: 'A', 'B', 'C', 'D', 'E' ## ## No pre-processing## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 15697, 15698, 15696, 15699, 15698 ## Resampling results:## ##   Accuracy   Kappa    ##   0.3902767  0.2203644## ## ","categories": [],
        "tags": ["Machine Learning","Ensembling","Data Science"],
        "url": "https://jairgs.github.io/machine-learning/",
        "teaser":null}]
